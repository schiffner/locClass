% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kmc.R
\name{kmc}
\alias{kmc}
\alias{kmc.data.frame}
\alias{kmc.default}
\alias{kmc.formula}
\alias{kmc.matrix}
\alias{update.kmc}
\title{K-Means Classification}
\usage{
kmc(x, ...)

\method{kmc}{formula}(formula, data, ..., subset, na.action)

\method{kmc}{data.frame}(x, ...)

\method{kmc}{matrix}(x, grouping, ..., subset, na.action = na.fail)

\method{kmc}{default}(x, grouping, K = 2, wf = c("biweight", "cauchy",
  "cosine", "epanechnikov", "exponential", "gaussian", "optcosine",
  "rectangular", "triangular"), bw, k, nn.only = TRUE, nstart = 1, ...)

update.kmc(object, wf = c("biweight", "cauchy", "cosine", "epanechnikov",
  "exponential", "gaussian", "optcosine", "rectangular", "triangular"), bw, k,
  nn.only, ...)
}
\arguments{
\item{x}{(Required if no \code{formula} is given as principal argument.) A \code{matrix} or \code{data.frame} or \code{Matrix} containing the explanatory variables.}

\item{formula}{A \code{formula} of the form \code{groups ~ x1 + x2 + \dots}, that is, the response is the 
grouping \code{factor} and the right hand side specifies the (usually non-\code{factor}) discriminators.}

\item{data}{A \code{data.frame} from which variables specified in \code{formula} are to be taken.}

\item{subset}{An index vector specifying the cases to be used in the training sample. (NOTE: If given, this argument must be named.)}

\item{na.action}{A function to specify the action to be taken if NAs are found. The default action is first
the \code{na.action} setting of \code{\link{options}} and second \code{\link{na.fail}} if that is unset. 
An alternative is \code{\link{na.omit}}, which leads to rejection of cases with missing values on any required 
variable. (NOTE: If given, this argument must be named.)}

\item{grouping}{(Required if no \code{formula} is given as principal argument.) A \code{factor} specifying the class membership for each observation.}

\item{K}{The number of prototypes per class, either a single number or a vector of length equal to the number of classes. 
The numbers of centers have to be in the same order as the levels of grouping. Default is \code{K = 2}.}

\item{wf}{A window function which is used to calculate weights that are introduced into 
the fitting process. Either a character string or a function, e.g. \code{wf = function(x) exp(-x)}.
For details see the documentation for \code{\link[=biweight]{wfs}}.}

\item{bw}{(Required only if \code{wf} is a string.) The bandwidth parameter of the window function. (See \code{\link[=biweight]{wfs}}.)}

\item{k}{(Required only if \code{wf} is a string.) The number of nearest neighbors of the decision boundary to be used in the fitting process. (See \code{\link[=biweight]{wfs}}.)}

\item{nn.only}{(Required only if \code{wf} is a string indicating a window function with infinite support and if \code{k} is specified.) Should
only the \code{k} nearest neighbors or all observations receive positive weights? (See \code{\link[=biweight]{wfs}}.)}

\item{nstart}{The number of random starts of the K-means algorithm. See \code{\link{kmeans}}.}

\item{object}{An object of class \code{"kmc"}.}

\item{\dots}{Further arguments to be passed to \code{\link{kmeans}}.}
}
\value{
An object of class \code{"kmc"} containing the following components:
\item{counts}{The number of observations per class.}
\item{x}{A \code{matrix} of prototypes.}
\item{grouping}{A \code{factor} specifying the class membership for each prototype.}
\item{lev}{The class labels (the levels of \code{grouping}).}
\item{N}{The number of training observations.}
\item{K}{The (used) number of prototypes per class.}
\item{call}{The (matched) function call.}
}
\description{
Classification based on K-means clustering within the classes.
}
\details{
Prototype methods represent the training data by a set of points in feature
space. Each prototype has an associated class label, and classification of a query
point \eqn{x} is made to the class of the closest prototype.

In order to use K-means clustering for classification of labeled data K-means 
clustering is applied to the training data of each class separately, using the number
of prototypes per class which is specified by \code{K} and defaults to 2.

Usually for a test observation the class label of the closest prototype is predicted.
But it is also possible to use more than 1 prototype and to weigh the influence of the prototypes on 
the classification according to their distances from the observation to be classified.
This is controlled by the arguments \code{wf}, \code{k}, \code{bw} and \code{nn.only}
(see \code{\link[=biweight]{wfs}}).

The name of the window function (\code{wf}) can be specified as a character string.
In this case the window function is generated internally in \code{kmc}. Currently
supported are \code{"biweight"}, \code{"cauchy"}, \code{"cosine"}, \code{"epanechnikov"}, 
\code{"exponential"}, \code{"gaussian"}, \code{"optcosine"}, \code{"rectangular"} and 
\code{"triangular"}.

Moreover, it is possible to generate the window functions mentioned above in advance 
(see \code{\link[=biweight]{wfs}}) and pass them to \code{kmc}.

Any other function implementing a window function can also be used as \code{wf} argument.
This allows the user to try own window functions.
See help on \code{\link[=biweight]{wfs}} for details.

It may be useful to \code{\link{scale}} the data first.

If the predictor variables include factors, the formula interface must be used in order 
to get a correct model matrix.
}
\examples{
# generate waveform data
library(mlbench)
data.train <- as.data.frame(mlbench.waveform(300))

# 3 centers per class
object <- kmc(classes ~ ., data = data.train, K = 3, wf = "rectangular", k = 1)
object <- kmc(data.train[,-22], data.train$classes, K = 3, wf = "rectangular", k = 1)

# 2 centers in class 1, 3 centers in class 2, 4 centers in class 3 
object <- kmc(classes ~ ., data = data.train, K = c(2,3,4), wf = "rectangular", k = 1)
object <- kmc(data.train[,-22], data.train$classes, K = c(2,3,4), wf = "rectangular", k = 1)

}
\references{
T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning: Data Mining,
Inference, and Prediction. Springer Series in Statistics. Springer, New York, 2001.
}
\seealso{
\code{\link{predict.kmc}}, \code{\link{kmeans}}.
}
\keyword{classif}
\keyword{multivariate}

