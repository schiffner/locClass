% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dannet.R
\name{dannet}
\alias{dannet}
\alias{dannet.data.frame}
\alias{dannet.default}
\alias{dannet.formula}
\alias{dannet.matrix}
\title{Discriminant Adaptive Neural Network}
\usage{
dannet(x, ...)

\method{dannet}{formula}(formula, data, weights, ..., subset, na.action,
  contrasts = NULL)

\method{dannet}{data.frame}(x, ...)

\method{dannet}{matrix}(x, y, weights = rep(1, nrow(x)), ..., subset,
  na.action = na.fail)

\method{dannet}{default}(x, y, wf = c("biweight", "cauchy", "cosine",
  "epanechnikov", "exponential", "gaussian", "optcosine", "rectangular",
  "triangular"), bw, k, nn.only, itr = 3, weights = rep(1, nrow(x)),
  reps = 1, ...)
}
\arguments{
\item{x}{(Required if no \code{formula} is given as principal argument.) A \code{matrix} or \code{data.frame} containing the explanatory variables.}

\item{formula}{A formula of the form \code{groups ~ x1 + x2 + \dots}, that is, the response
is the grouping \code{factor} and the right hand side specifies the (normally non-\code{factor})
discriminators.}

\item{data}{A \code{data.frame} from which variables specified in \code{formula} are to be taken.}

\item{weights}{Initial observation weights (defaults to a vector of 1s).}

\item{subset}{An index vector specifying the cases to be used in the training sample. (NOTE: If given, this argument must be named.)}

\item{na.action}{A function to specify the action to be taken if NAs are found. The default action is first
the \code{na.action} setting of \code{\link{options}} and second \code{\link{na.fail}} if that is unset. 
An alternative is \code{\link{na.omit}}, which leads to rejection of cases with missing values on any required 
variable. (NOTE: If given, this argument must be named.)}

\item{contrasts}{A list of contrasts to be used for some or all of the factors appearing as variables in the model formula.}

\item{y}{(Required if no \code{formula} is given as principal argument.) A \code{factor} specifying
the class membership for each observation.}

\item{wf}{A window function which is used to calculate weights that are introduced into 
the fitting process. Either a character string or a function, e.g. \code{wf = function(x) exp(-x)}.
For details see the documentation for \code{\link[=biweight]{wfs}}.}

\item{bw}{(Required only if \code{wf} is a string.) The bandwidth parameter of the window function. (See \code{\link[=biweight]{wfs}}.)}

\item{k}{(Required only if \code{wf} is a string.) The number of nearest neighbors of the decision boundary to be used in the fitting process. 
(See \code{\link[=biweight]{wfs}}.)}

\item{nn.only}{(Required only if \code{wf} is a string indicating a window function with infinite support and if \code{k} is specified.) Should
only the \code{k} nearest neighbors or all observations receive positive weights? (See \code{\link[=biweight]{wfs}}.)}

\item{itr}{Number of iterations for model fitting, defaults to 3. See also the Details section.}

\item{reps}{Neural networks are fitted repeatedly (\code{reps} times) for different initial values and the solution with largest likelihood
value is kept. Defaults to 1. (\code{reps} larger one does not make sense if \code{Wts} is specified.)}

\item{\dots}{Further arguments to \code{\link[nnet]{nnet}}.}
}
\value{
An object of class \code{"dannet"} or \code{"dannet.formula"} inheriting from \code{"nnet"}. A \code{list} mostly containing internal structure,
but with the following components: 
 \item{wts}{The best set of weights found.}
 \item{value}{Value of fitting criterion plus weight decay term.}
 \item{fitted.values}{The fitted values for the training data.}
 \item{residuals}{The residuals for the training data.}
 \item{convergence}{1 if the maximum number of iterations was reached, otherwise 0.}
 \item{weights}{A list of length \code{itr + 1}. The initial observation weights (a vector of 1s if none were given) and the observation
  weights calculated in the individual iterations.}
 \item{itr}{The number of iterations used.}
 \item{wf}{The window function used. Always a function, even if the input was a string.}
 \item{bw}{(Only if \code{wf} is a string or was generated by means of one of the functions documented in \code{\link[=biweight]{wfs}}.) 
  The bandwidth used, \code{NULL} if \code{bw} was not specified.}
\item{k}{(Only if \code{wf} is a string or was generated by means of one of the functions documented in \code{\link[=biweight]{wfs}}.) 
  The number of nearest neighbors used, \code{NULL} if \code{k} was not specified.}
 \item{nn.only}{(Logical. Only if \code{wf} is a string or was generated by means of one of the functions documented in \code{\link[=biweight]{wfs}} and if \code{k} was
 specified.) \code{TRUE} if only the \code{k} nearest neighbors recieve a positive weight, \code{FALSE} otherwise.}
 \item{adaptive}{(Logical.) \code{TRUE} if the bandwidth of \code{wf} is adaptive to the local density of data points, \code{FALSE} if the bandwidth
  is fixed.}
 \item{call}{The (matched) function call.}
}
\description{
A local version of a single-hidden-layer neural network for classification that puts increased emphasis on a good model fit near the decision boundary.
}
\details{
The idea of Hand and Vinciotti (2003) to put increased weight on observations near the decision boundary is generalized to the multiclass case and applied to 
neural networks.
Since the decision boundary is not known in advance an iterative procedure is required.
First, an unweighted neural network is fitted to the data. 
Based on the differences between the two largest estimated posterior probabilities observation weights are calculated.
Then a weighted neural network (see \code{\link[nnet]{nnet}}) from package \pkg{nnet} is fitted using these weights. 
Calculation of weights and model fitting is done several times in turn. 
The number of iterations is determined by the \code{itr}-argument that defaults to 3.

The name of the window function (\code{wf}) can be specified as a character string.
In this case the window function is generated internally in \code{dalda}. Currently
supported are \code{"biweight"}, \code{"cauchy"}, \code{"cosine"}, \code{"epanechnikov"}, 
\code{"exponential"}, \code{"gaussian"}, \code{"optcosine"}, \code{"rectangular"} and 
\code{"triangular"}.

Moreover, it is possible to generate the window functions mentioned above in advance 
(see \code{\link[=biweight]{wfs}}) and pass them to \code{dalda}.

Any other function implementing a window function can also be used as \code{wf} argument.
This allows the user to try own window functions.
See help on \code{\link[=biweight]{wfs}} for details.

If the predictor variables include factors, the formula interface must be used in order 
to get a correct model matrix.

In contrast to \code{\link[nnet]{nnet}} this function is only appropriate for classification
problems. As response in \code{formula} only factors are allowed. If the
response is not a factor, it is coerced to a factor with a warning.
An appropriate classification network is constructed; this has one output and entropy fit if the
number of levels is two, and a number of outputs equal to the number
of classes and a softmax output stage for more levels. 
If you use the default method, you get only meaningful results if \code{y} is a 0-1 class indicator
matrix.

Optimization is done via the BFGS method of \code{\link{optim}}.
}
\examples{
 fit <- dannet(Species ~ Sepal.Length + Sepal.Width, data = iris, size = 2, rang = 0.1, maxit = 200, bw = 2)
 pred <- predict(fit)
 mean(pred$class != iris$Species)
}
\references{
Hand, D. J., Vinciotti, V. (2003), Local versus global models for classification problems: 
Fitting models where it matters, \emph{The American Statistician}, \bold{57(2)} 124--130.

Ripley, B. D. (1996) \emph{Pattern Recognition and Neural Networks}. Cambridge.

Venables, W. N. and Ripley, B. D. (2002) \emph{Modern Applied Statistics with S}. Fourth edition. Springer.
}
\seealso{
\code{\link{predict.dannet}}, \code{\link[nnet]{nnet}} and 
\code{\link{dalr}} for discriminant adaptive logistic regression.
}
\keyword{classif}
\keyword{multivariate}

